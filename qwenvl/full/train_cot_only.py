#!/usr/bin/env python
# coding=utf-8
"""
Train Qwen2.5-VL on CoT-style JSON data with lazy loading.
å»¶è¿ŸåŠ è½½ç‰ˆæœ¬ - å›¾ç‰‡æŒ‰éœ€åŠ è½½ï¼Œé€‚åˆå¤§è§„æ¨¡æ•°æ®é›†

ç‰¹æ€§ï¼š
- å»¶è¿ŸåŠ è½½ï¼šå¯åŠ¨å¿«é€Ÿï¼Œå›¾ç‰‡æŒ‰éœ€åŠ è½½
- LRUç¼“å­˜ï¼šè‡ªåŠ¨ç®¡ç†å†…å­˜ä¸­çš„å›¾ç‰‡ç¼“å­˜
- åˆ†å¸ƒå¼è®­ç»ƒï¼šæ”¯æŒå¤šGPU/å¤šèŠ‚ç‚¹è®­ç»ƒ
- ä¼˜åŒ–æ€§èƒ½ï¼šé’ˆå¯¹å›ºå®šæ•°æ®æ ¼å¼ä¼˜åŒ–

ä½¿ç”¨æ–¹æ³•ï¼š
  python train_cot_pdsh.py --cot_files data.json
"""

# ==================== é»˜è®¤é…ç½®åŒºåŸŸ ====================
# è¿™äº›æ˜¯é»˜è®¤å€¼ï¼Œå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
DEFAULT_CONFIG = {
    "base_dir": "/apdcephfs_gy2/share_302507476/xiaodayang/SpatialLogic/data/clips",  # é¡¹ç›®æ ¹ç›®å½•
    "model_path": "/apdcephfs_gy2/share_302507476/xiaodayang/SpatialLogic/ckpt/qwenvl/original/Qwen2.5-VL-7B-Instruct",  # æ¨¡å‹è·¯å¾„
    "output_dir": "/apdcephfs_gy2/share_302507476/xiaodayang/SpatialLogic/ckpt/qwenvl/full/cot_only",  # è¾“å‡ºç›®å½•
    "log_dir": "/apdcephfs_gy2/share_302507476/xiaodayang/SpatialLogic/log/cot_only",  # TensorBoard æ—¥å¿—ç›®å½•
    "cot_files": [],  # COTæ•°æ®æ–‡ä»¶åˆ—è¡¨ï¼Œç”¨æˆ·å¿…é¡»é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®š
    "epochs": 2,                    # è®­ç»ƒè½®æ•°
    "batch_size": 1,                # æ‰¹æ¬¡å¤§å°ï¼ˆä¿æŒä¸º1ï¼‰
    "learning_rate": 2e-7,          # å­¦ä¹ ç‡ï¼ˆè¿›ä¸€æ­¥é™ä½åˆ°1e-7ï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰
    "max_samples": 0,             # æœ€å¤§æ ·æœ¬æ•°ï¼ˆ0è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
    "max_steps": 0,                 # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼ˆè®¾ä¸º0è¡¨ç¤ºæ ¹æ®æ•°æ®é‡è‡ªåŠ¨è®¡ç®—ï¼‰
    "gradient_accumulation_steps": 2,   # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆå‡å°‘åˆ°2ï¼Œæœ€å¤§ç¨‹åº¦èŠ‚çœæ˜¾å­˜ï¼‰
    "warmup_steps": 1000,            # é¢„çƒ­æ­¥æ•°
    "save_steps": 3000,              # ä¿å­˜æ£€æŸ¥ç‚¹æ­¥æ•°
    "logging_steps": 10,            # æ—¥å¿—è®°å½•æ­¥æ•°ï¼ˆå‡å°‘æ§åˆ¶å°è¾“å‡ºé¢‘ç‡ï¼‰
    "max_grad_norm": 0.3,           # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé™ä½åˆ°0.3ï¼Œæ›´ä¿å®ˆçš„æ¢¯åº¦æ§åˆ¶ï¼‰
    "weight_decay": 0.01,           # æƒé‡è¡°å‡ï¼ˆå…¨é‡å¾®è°ƒä½¿ç”¨æ›´å¤§çš„æƒé‡è¡°å‡ï¼‰
    "max_target_length": 1500,      # ç›®æ ‡æ–‡æœ¬æœ€å¤§é•¿åº¦ï¼ˆå‡å°‘åˆ°2000è¿›ä¸€æ­¥èŠ‚çœæ˜¾å­˜ï¼‰
}
# ===================================================

import os
# Reduce thread contention / avoid OpenBLAS OpenMP loop warnings
os.environ.setdefault("OPENBLAS_NUM_THREADS", "1")
os.environ.setdefault("OMP_NUM_THREADS", "1")
os.environ.setdefault("MKL_NUM_THREADS", "1")
os.environ.setdefault("NUMEXPR_NUM_THREADS", "1")
os.environ.setdefault("TORCH_NUM_THREADS", "1")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")


import json
import argparse
import time
from typing import List, Dict, Any
from tqdm import tqdm
from collections import OrderedDict

# å¯é€‰å¿«é€ŸJSONè§£æ
try:
    import orjson as _orjson
except Exception:
    _orjson = None

import torch
try:
    torch.set_num_threads(int(os.environ.get("TORCH_NUM_THREADS", "1")))
    if hasattr(torch, "set_num_interop_threads"):
        torch.set_num_interop_threads(1)
except Exception:
    pass

from PIL import Image
from torch.utils.data import Dataset
from transformers import (
    Qwen2_5_VLForConditionalGeneration,
    AutoProcessor,
    Trainer,
    TrainingArguments,
    TrainerCallback,
)
# TensorBoard å¯¼å…¥ï¼Œå…¼å®¹ä¸åŒç¯å¢ƒ
try:
    from torch.utils.tensorboard import SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    try:
        from tensorboardX import SummaryWriter
        TENSORBOARD_AVAILABLE = True
    except ImportError:
        TENSORBOARD_AVAILABLE = False
        print("Warning: tensorboard not available, TensorBoard logging will be disabled")

# å¯¼å…¥accelerateç›¸å…³æ¨¡å—
try:
    from accelerate import Accelerator
    from accelerate.utils import set_seed
    ACCELERATE_AVAILABLE = True
except ImportError:
    ACCELERATE_AVAILABLE = False
    print("Warning: accelerate not available, falling back to basic training")


# å»¶è¿ŸåŠ è½½æ•°æ®é›† - å›¾ç‰‡æŒ‰éœ€åŠ è½½ï¼Œé¿å…é¢„å¤„ç†


# å·²ç§»é™¤ LRU ç¼“å­˜é€»è¾‘


class LazyCoTDataset(Dataset):
    """å»¶è¿ŸåŠ è½½ç‰ˆæœ¬çš„ CoT æ•°æ®é›†ï¼Œé¿å…å¯åŠ¨æ—¶é¢„å¤„ç†æ‰€æœ‰å›¾ç‰‡"""
    
    def __init__(self, processor, cot_files: List[str], base_dir: str, max_samples: int = None, 
                 max_target_length: int = 3000):
        self.processor = processor
        self.max_target_length = max_target_length
        self.base_dir = base_dir
        self.max_samples = max_samples  # ä¿å­˜åŸå§‹å€¼
        
        # å·²ç§»é™¤å›¾ç‰‡ç¼“å­˜é€»è¾‘ï¼šå§‹ç»ˆæŒ‰éœ€è¯»å–å›¾ç‰‡
        self.image_cache = None
        
        # ä»…åŠ è½½å…ƒæ•°æ®ï¼Œä¸é¢„å¤„ç†å›¾ç‰‡
        self.samples_metadata = []
        
        print(f"ğŸš€ å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼šä»…åŠ è½½æ ·æœ¬å…ƒæ•°æ®...")
        start_time = time.time()
        file_names = [os.path.basename(f) for f in cot_files]
        print(f"ğŸ“ è®¡åˆ’åŠ è½½ {len(cot_files)} ä¸ªæ–‡ä»¶: {file_names}")
        print(f"â° å¼€å§‹æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(start_time))}")
        

        for jf in cot_files:
            jf_abs = jf if os.path.isabs(jf) else os.path.abspath(jf)
            if not os.path.exists(jf_abs):
                print(f"âš ï¸ [WARN] CoT file not found: {jf_abs}")
                continue
                
            # åŠ è½½JSONæ–‡ä»¶
            with open(jf_abs, 'rb') as f:
                if _orjson is not None:
                    data = _orjson.loads(f.read())
                else:
                    import json
                    data = json.load(f)
                    print("åŠ è½½jsonæ–‡ä»¶å®Œæˆ", flush=True)

            total_items = len(data) if isinstance(data, list) else 0
            print(f"ğŸ“¦ æ–‡ä»¶: {os.path.basename(jf_abs)} | æ ·æœ¬æ•°: {total_items}", flush=True)

            added_samples = 0

            iterator = data
            try:
                iterator = tqdm(data, desc=f"å¤„ç† {os.path.basename(jf_abs)}", unit="æ ·æœ¬")
            except Exception:
                pass

            for item in iterator:
                messages = item.get('messages', [])
                target = item.get('target', {})
                
                # å›ºå®šæ ¼å¼ï¼šç¬¬ä¸€ä¸ªæ¶ˆæ¯çš„å‰ä¸¤ä¸ªcontentæ˜¯å›¾ç‰‡
                content = messages[0]['content']
                img1_path = content[0]['image'].replace('_/','./')
                img2_path = content[1]['image'].replace('_/','./')
                
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if not os.path.isabs(img1_path):
                    img1_path = os.path.join(base_dir, img1_path)
                    content[0]['image'] = img1_path
                if not os.path.isabs(img2_path):
                    img2_path = os.path.join(base_dir, img2_path)
                    content[1]['image'] = img2_path
                
                image_paths = [img1_path, img2_path]

                closer = target.get("closer to completion", "")
                spatial_details = target.get("spatial details") 
                target_text = f"spatial details: {spatial_details}\ncloser to completion: {closer}"
                
                metadata = {
                    'messages': messages,
                    'target_text': target_text,
                    'image_paths': image_paths,  # é¢„å¤„ç†çš„å›¾ç‰‡è·¯å¾„åˆ—è¡¨
                }
                
                self.samples_metadata.append(metadata)
                added_samples += 1
                
                # è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°æ—¶åœæ­¢
                if self.max_samples and len(self.samples_metadata) >= self.max_samples:
                    break
            
            if self.max_samples and len(self.samples_metadata) >= self.max_samples:
                break

            print(
                f"âœ… å®Œæˆ {os.path.basename(jf_abs)} | å…ƒæ•°æ®æˆåŠŸ: {added_samples}",
                flush=True,
            )
        
        end_time = time.time()
        elapsed_time = end_time - start_time
        print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆ: {len(self.samples_metadata)} ä¸ªæ ·æœ¬")
        print(f"ğŸ”§ å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼šå›¾ç‰‡æŒ‰éœ€åŠ è½½ï¼Œæ— ç¼“å­˜")
        print(f"â° ç»“æŸæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(end_time))}")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’ ({elapsed_time/60:.2f} åˆ†é’Ÿ)")
    
    def __len__(self):
        return len(self.samples_metadata)
    

    
    def _load_image_cached(self, image_path: str):
        """å›¾ç‰‡åŠ è½½"""
        try:
            img = Image.open(image_path)
            # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ ¼å¼ï¼Œå¤„ç†å„ç§é¢œè‰²æ¨¡å¼
            if img.mode != 'RGB':
                if img.mode in ('RGBA', 'LA'):
                    # æœ‰é€æ˜é€šé“çš„å›¾ç‰‡ï¼Œå…ˆè½¬æ¢ä¸ºç™½è‰²èƒŒæ™¯
                    background = Image.new('RGB', img.size, (255, 255, 255))
                    if img.mode == 'RGBA':
                        background.paste(img, mask=img.split()[-1])  # ä½¿ç”¨alphaé€šé“ä½œä¸ºmask
                    else:  # LA mode
                        background.paste(img.convert('RGB'))
                    img = background
            else:
                    # å…¶ä»–æ¨¡å¼ç›´æ¥è½¬æ¢
                    img = img.convert('RGB')
            
            return img
        except Exception as e:
            return None

    def __getitem__(self, idx):
        metadata = self.samples_metadata[idx]
        
        # ä½¿ç”¨é¢„å¤„ç†çš„ç›®æ ‡æ–‡æœ¬
        target_text = metadata['target_text']
        
        # é™åˆ¶ç›®æ ‡æ–‡æœ¬é•¿åº¦
        if len(target_text) > self.max_target_length:
            target_text = target_text[:self.max_target_length]
        
        # ä½¿ç”¨é¢„å¤„ç†çš„å›¾ç‰‡è·¯å¾„å¿«é€ŸåŠ è½½
        image_inputs = []
        for image_path in metadata['image_paths']:
            img = self._load_image_cached(image_path)
            if img is None:
                # å›¾ç‰‡åŠ è½½å¤±è´¥ï¼Œè·³è¿‡è¯¥æ ·æœ¬
                return None
            else:
                image_inputs.append(img)
        
        # å¤„ç†å¯¹è¯
        messages = metadata['messages']
        chat_prompt = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        # ä¸ add_generation_prompt=True ä¿æŒä¸€è‡´ï¼šä¸å†æ‰‹åŠ¨è¿½åŠ  "\nassistant\n"
        full_text = chat_prompt + target_text
        
        inputs = self.processor(
            text=full_text, 
            images=image_inputs, 
            return_tensors='pt', 
            padding=False,
            truncation=False
        )
        
        input_ids = inputs.input_ids.squeeze(0)
        attention_mask = inputs.attention_mask.squeeze(0)
        
        # è®¡ç®—æ ‡ç­¾ï¼šå‰ç¼€é•¿åº¦å³æ¨¡æ¿äº§ç”Ÿçš„ chat_prompt çš„ token æ•°
        prefix_len = self.processor.tokenizer(chat_prompt, return_tensors='pt').input_ids.shape[-1]
        labels = input_ids.clone()
        labels[:prefix_len] = -100
        
        # ç¡®ä¿æœ‰æ•ˆæ ‡ç­¾
        valid_label_count = (labels != -100).sum().item()
        if valid_label_count == 0:
            labels[-3:] = input_ids[-3:]
        
        if hasattr(self.processor.tokenizer, 'pad_token_id') and self.processor.tokenizer.pad_token_id is not None:
            labels[labels == self.processor.tokenizer.pad_token_id] = -100
        
        # æ”¶é›†è§†è§‰ç›¸å…³å¼ é‡ï¼ˆå¦‚pixel_valuesã€image_grid_thwç­‰ï¼‰ï¼Œå¹¶å»é™¤æ‰¹ç»´åº¦
        extra = {}
        for k, v in inputs.items():
            if k not in ["input_ids", "attention_mask"] and torch.is_tensor(v):
                if v.dim() > 0 and v.size(0) == 1:
                    extra[k] = v.squeeze(0).cpu()
                else:
                    extra[k] = v.cpu()
        # å¤„ç† image_grid_thw
        if 'image_grid_thw' in inputs and 'image_grid_thw' not in extra:
            grid = inputs['image_grid_thw']
            # æ ¹æ®æµ‹è¯•ç»“æœï¼Œprocessorè¿”å›çš„æ ¼å¼å›ºå®šä¸º (n, 3)ï¼Œç›´æ¥ä½¿ç”¨
            if not torch.is_tensor(grid):
                grid = torch.tensor(grid, dtype=torch.long)
            extra['image_grid_thw'] = grid.cpu()
        
        # ç¡®ä¿å¼ é‡åœ¨CPUä¸Šï¼Œè®©Trainerå’Œaccelerateå¤„ç†è®¾å¤‡åˆ†é…
        sample = {
            "input_ids": input_ids.cpu(), 
            "attention_mask": attention_mask.cpu(), 
            "labels": labels.cpu()
        }
        sample.update(extra)
        return sample
    
    def get_dataset_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        return {
            'total_samples': len(self.samples_metadata),
            'max_samples_limit': self.max_samples,
            'loading_mode': 'lazy_loading',
            'cache_enabled': False
        }


def create_collate_fn(processor):
    """
    åˆ›å»ºå¤„ç†å˜é•¿åºåˆ—çš„collateå‡½æ•°
    æ”¯æŒä¸åŒé•¿åº¦çš„input_idsã€attention_maskå’Œlabels

    è®¾è®¡è¯´æ˜ï¼š
    1. ä½¿ç”¨å·¥å‚å‡½æ•°æ¨¡å¼ï¼Œå¯ä»¥è®¿é—®processorçš„tokenizerä¿¡æ¯
    2. input_idsä½¿ç”¨æ­£ç¡®çš„pad_token_idå¡«å……ï¼Œè€Œä¸æ˜¯ç¡¬ç¼–ç çš„0
    3. ç»Ÿä¸€å¤„ç†paddingé€»è¾‘ï¼Œé¿å…åœ¨__getitem__ä¸­é‡å¤padding
    4. è¿‡æ»¤æ‰Noneå€¼ï¼ˆå½“å›¾ç‰‡åŠ è½½å¤±è´¥ä¸”é…ç½®ä¸ºè·³è¿‡æ—¶ï¼‰
    """
    # è®°å¿†æœ€è¿‘ä¸€æ¬¡å¯ç”¨æ ·æœ¬ï¼Œç”¨äºåœ¨æ•´æ‰¹æ— æ•ˆæ—¶å…œåº•
    last_good_sample = {"value": None}

    def collate_fn(batch):
        # è¿‡æ»¤æ‰Noneå€¼ï¼ˆå›¾ç‰‡åŠ è½½å¤±è´¥çš„æ ·æœ¬ï¼‰
        valid_batch = [b for b in batch if b is not None]
        if not valid_batch:
            if last_good_sample["value"] is not None:
                # ä½¿ç”¨æœ€è¿‘ä¸€æ¬¡æœ‰æ•ˆæ ·æœ¬å…œåº•ï¼Œé¿å…è®­ç»ƒä¸­æ–­
                print("âš ï¸  Batch contains only invalid samples, reusing last valid sample as fallback", flush=True)
                valid_batch = [last_good_sample["value"]]
            else:
                # é¦–æ‰¹å°±æ— æœ‰æ•ˆæ ·æœ¬ï¼Œåªè¿”å›ç©ºå¼ é‡ä»¥è·³è¿‡è¯¥æ­¥ï¼ˆä¿è¯å½¢çŠ¶åˆæ³•ï¼‰
                pad_token_id = processor.tokenizer.pad_token_id or 0
                empty_ids = torch.tensor([pad_token_id])
                empty_mask = torch.tensor([1])
                empty_labels = torch.tensor([-100])
                return {
                    "input_ids": empty_ids.unsqueeze(0),
                    "attention_mask": empty_mask.unsqueeze(0),
                    "labels": empty_labels.unsqueeze(0),
                }

        # è·å–æ‰€æœ‰æ ·æœ¬çš„é”®
        keys = valid_batch[0].keys()

        # æ‰¾åˆ°æœ€å¤§é•¿åº¦
        max_length = max(len(b['input_ids']) for b in valid_batch)

        # åˆå§‹åŒ–ç»“æœå­—å…¸
        result = {}

        for key in keys:
            if key in ['input_ids', 'attention_mask', 'labels']:
                # å¯¹äºéœ€è¦paddingçš„å¼ é‡
                padded_tensors = []
                for b in valid_batch:
                    tensor = b[key]
                    current_length = len(tensor)

                    if current_length < max_length:
                        # éœ€è¦padding
                        if key == 'labels':
                            # labelsç”¨-100å¡«å……
                            padding = torch.full((max_length - current_length,), -100, dtype=tensor.dtype)
                        elif key == 'input_ids':
                            # input_idsç”¨pad_token_idå¡«å……
                            pad_token_id = processor.tokenizer.pad_token_id or 0
                            padding = torch.full((max_length - current_length,), pad_token_id, dtype=tensor.dtype)
                        else:
                            # attention_maskç”¨0å¡«å……
                            padding = torch.zeros(max_length - current_length, dtype=tensor.dtype)

                        padded_tensor = torch.cat([tensor, padding])
                    else:
                        padded_tensor = tensor

                    padded_tensors.append(padded_tensor)

                # å †å æ‰€æœ‰paddedå¼ é‡
                result[key] = torch.stack(padded_tensors)
            else:
                # è§†è§‰æˆ–å…¶ä»–å¼ é‡
                if key == 'pixel_values':
                    # å°†æ¯ä¸ªæ ·æœ¬ä¸­çš„å›¾åƒç»´æ‹¼æ¥åˆ°ä¸€èµ·ï¼Œå½¢æˆ (sum_images, C, H, W)
                    result[key] = torch.cat([b[key] if b[key].dim() >= 3 else b[key].unsqueeze(0) for b in valid_batch], dim=0)
                elif key == 'image_grid_thw':
                    # ç»Ÿä¸€å°†æ¯ä¸ªæ ·æœ¬çš„ grid å˜æˆ (n, 3)ï¼Œç„¶ååœ¨æ ·æœ¬ç»´æ‹¼æ¥ -> (sum_images, 3)
                    fixed = []
                    for b in valid_batch:
                        g = b[key]
                        if torch.is_tensor(g):
                            if g.dim() == 1:
                                if g.numel() == 2:
                                    g = torch.tensor([1, int(g[0]), int(g[1])], dtype=torch.long).view(1, 3)
                                elif g.numel() == 3:
                                    g = g.view(1, 3).long()
                                else:
                                    raise ValueError(f"Unexpected image_grid_thw shape: {g.shape}")
                            elif g.dim() >= 2:
                                if g.size(-1) == 2:
                                    ones = torch.ones(*g.shape[:-1], 1, dtype=g.dtype)
                                    g = torch.cat([ones, g], dim=-1)
                                # å°†å¯èƒ½å­˜åœ¨çš„æ›´é«˜ç»´å‰ç¼€å±•å¹³æˆ (n, 3)
                                g = g.view(-1, 3).long()
                        else:
                            # list/tuple -> å¼ é‡
                            if isinstance(g, (list, tuple)):
                                def to_thw_list(x):
                                    if isinstance(x, (list, tuple)):
                                        if len(x) == 2 and all(isinstance(n, (int, float)) for n in x):
                                            return [1, int(x[0]), int(x[1])]
                                        if len(x) == 3 and all(isinstance(n, (int, float)) for n in x):
                                            return [int(x[0]), int(x[1]), int(x[2])]
                                        return [to_thw_list(e) for e in x]
                                    return x
                                g_list = to_thw_list(g)
                                g = torch.tensor(g_list, dtype=torch.long)
                                g = g.view(-1, 3)
                            else:
                                raise TypeError("image_grid_thw must be Tensor or list/tuple")
                        fixed.append(g)
                    result[key] = torch.cat(fixed, dim=0)
                else:
                    # å…¶ä»–å®šé•¿å¼ é‡ï¼Œç›´æ¥å †å 
                    result[key] = torch.stack([b[key] for b in valid_batch])

        # æ›´æ–°æœ€è¿‘ä¸€æ¬¡å¯ç”¨æ ·æœ¬
        try:
            last_good_sample["value"] = {
                k: (v.detach().clone() if torch.is_tensor(v) else v)
                for k, v in valid_batch[-1].items()
            }
        except Exception:
            pass

        return result

    return collate_fn


 

class TensorBoardCallback(TrainerCallback):
    """TensorBoard å›è°ƒï¼Œè®°å½•è¯¦ç»†çš„è®­ç»ƒä¿¡æ¯"""
    
    def __init__(self, log_dir=None):
        self.log_dir = log_dir or "/apdcephfs_gy2/share_302507476/xiaodayang/SpatialLogic/log"
        self.writer = None
        
    def on_train_begin(self, args, state, control, **kwargs):
        """è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ– TensorBoard writer"""
        if state.is_local_process_zero and TENSORBOARD_AVAILABLE:
            try:
                self.writer = SummaryWriter(self.log_dir)
                print(f"ğŸ“Š TensorBoard æ—¥å¿—å°†ä¿å­˜åˆ°: {self.log_dir}")
            except Exception as e:
                print(f"âš ï¸  TensorBoard åˆå§‹åŒ–å¤±è´¥: {e}")
                self.writer = None
    
    def on_step_end(self, args, state, control, **kwargs):
        """æ¯æ­¥ç»“æŸæ—¶è®°å½•è®­ç»ƒä¿¡æ¯"""
        if state.is_local_process_zero and self.writer is not None:
            try:
                # è®°å½•å­¦ä¹ ç‡
                if hasattr(state, 'learning_rate'):
                    self.writer.add_scalar('train/learning_rate', state.learning_rate, state.global_step)
                
                # è®°å½•è®­ç»ƒæŸå¤±
                if hasattr(state, 'log_history') and state.log_history:
                    latest_log = state.log_history[-1]
                    if 'loss' in latest_log:
                        self.writer.add_scalar('train/loss', latest_log['loss'], state.global_step)
            except Exception as e:
                print(f"âš ï¸  TensorBoard è®°å½•å¤±è´¥: {e}")
    
    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸæ—¶å…³é—­ TensorBoard writer"""
        if state.is_local_process_zero and self.writer is not None:
            try:
                self.writer.close()
                print(f"ğŸ“Š TensorBoard æ—¥å¿—å·²ä¿å­˜åˆ°: {self.log_dir}")
            except Exception as e:
                print(f"âš ï¸  TensorBoard å…³é—­å¤±è´¥: {e}")





class GradientNaNCallback(TrainerCallback):
    """
    æ¢¯åº¦æ•°å€¼å¼‚å¸¸æ£€æµ‹å›è°ƒ
    
    åŠŸèƒ½ï¼š
    1. æ£€æµ‹å¹¶ä¿®å¤NaN/Infæ¢¯åº¦ï¼Œé˜²æ­¢è®­ç»ƒå´©æºƒ
    2. ä¸é‡å¤Trainerå†…ç½®çš„æ¢¯åº¦è£å‰ªåŠŸèƒ½
    
    æ³¨æ„ï¼š
    - æ¢¯åº¦è£å‰ªç”±TrainingArguments.max_grad_normè‡ªåŠ¨å¤„ç†
    - æ­¤å›è°ƒåœ¨optimizer.step()ä¹‹åæ‰§è¡Œï¼Œä¸»è¦ç”¨äºæ£€æµ‹å’Œæ—¥å¿—è®°å½•
    - å¯¹äºçœŸæ­£çš„NaN/Infæ¢¯åº¦ä¿®å¤ï¼Œå»ºè®®åœ¨æ›´æ—©çš„é˜¶æ®µï¼ˆå¦‚training_stepä¸­ï¼‰å¤„ç†
    """
    def on_step_end(self, args, state, control, **kwargs):
        model = kwargs.get("model")
        if model is None:
            return
            
        # æ£€æŸ¥NaNå’ŒInfæ¢¯åº¦ï¼Œåªå¤„ç†æ•°å€¼å¼‚å¸¸ï¼Œæ¢¯åº¦è£å‰ªç”±Trainerå†…ç½®åŠŸèƒ½å¤„ç†
        for name, p in model.named_parameters():
            if p.requires_grad and p.grad is not None:
                grad = p.grad.data
                if torch.isnan(grad).any():
                    print(f"âš ï¸  NaN gradient detected at {name}, replacing with zeros")
                    p.grad.data = torch.where(torch.isnan(grad), torch.zeros_like(grad), grad)
                elif torch.isinf(grad).any():
                    print(f"âš ï¸  Inf gradient detected at {name}, replacing with zeros")
                    p.grad.data = torch.where(torch.isinf(grad), torch.zeros_like(grad), grad)



def main():
    parser = argparse.ArgumentParser(description="Full Fine-tune Qwen2.5-VL on CoT JSON")
    parser.add_argument("--cot_files", nargs="+", default=DEFAULT_CONFIG["cot_files"], 
                       help="COTæ•°æ®æ–‡ä»¶åˆ—è¡¨ (æ”¯æŒå¤šä¸ªæ–‡ä»¶ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œå¦‚: cotdata/cot_50.0.json cotdata/cot_60.0.json)")
    parser.add_argument("--base_dir", type=str, default=DEFAULT_CONFIG["base_dir"], help="Base dir to resolve relative image paths (should contain testset)")
    parser.add_argument("--model_path", type=str, default=DEFAULT_CONFIG["model_path"], help="Local model dir")
    parser.add_argument("--output_dir", type=str, default=DEFAULT_CONFIG["output_dir"], help="Output dir")
    parser.add_argument("--epochs", type=int, default=DEFAULT_CONFIG["epochs"], help="Number of training epochs")
    parser.add_argument("--batch_size", type=int, default=DEFAULT_CONFIG["batch_size"], help="Training batch size")
    parser.add_argument("--learning_rate", type=float, default=DEFAULT_CONFIG["learning_rate"], help="Learning rate")
    parser.add_argument("--max_samples", type=int, default=DEFAULT_CONFIG["max_samples"], 
                       help="æœ€å¤§æ ·æœ¬æ•°é™åˆ¶ (è®¾ä¸º0æˆ–è´Ÿæ•°è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œç”¨äºé¿å…OOM)")
    parser.add_argument("--max_steps", type=int, default=DEFAULT_CONFIG["max_steps"], help="Maximum training steps")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=DEFAULT_CONFIG["gradient_accumulation_steps"], help="Gradient accumulation steps")
    parser.add_argument("--warmup_steps", type=int, default=DEFAULT_CONFIG["warmup_steps"], help="Warmup steps")
    parser.add_argument("--save_steps", type=int, default=DEFAULT_CONFIG["save_steps"], help="Save checkpoint steps")
    parser.add_argument("--logging_steps", type=int, default=DEFAULT_CONFIG["logging_steps"], help="Logging steps")
    parser.add_argument("--max_grad_norm", type=float, default=DEFAULT_CONFIG["max_grad_norm"], help="Max gradient norm")
    parser.add_argument("--weight_decay", type=float, default=DEFAULT_CONFIG["weight_decay"], help="Weight decay")
    parser.add_argument("--from_pretrained", action="store_true", help="Whether to load from a pretrained model")
    parser.add_argument("--accelerate", action="store_true", help="Whether to use accelerate for distributed training")
    parser.add_argument("--deepspeed", type=str, default=None, help="Path to DeepSpeed config file")
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--max_target_length", type=int, default=3000,
                       help="Maximum target text length before truncation (default: 3000 for optimal memory usage)")
    parser.add_argument("--dataloader_workers", type=int, default=0,
                       help="Number of PyTorch DataLoader workers for image decoding (default: 0 to avoid multiprocessing issues)")
    parser.add_argument("--log_dir", type=str, default=DEFAULT_CONFIG["log_dir"],
                       help="Directory to save TensorBoard logs (default: from DEFAULT_CONFIG)")
    args = parser.parse_args()

    print(f"âœ… å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼ˆå›¾ç‰‡æŒ‰éœ€åŠ è½½ï¼‰")

    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    
    # åˆå§‹åŒ–acceleratorï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
    if ACCELERATE_AVAILABLE and args.accelerate:
        accelerator = Accelerator()
        print(f"Using accelerate for distributed training")
        device = accelerator.device
        use_cuda = device.type == 'cuda'
    else:
        accelerator = None
        use_cuda = torch.cuda.is_available()
        device = torch.device('cuda' if use_cuda else 'cpu')
    
    dtype = torch.float32  # å¼ºåˆ¶ä½¿ç”¨float32æé«˜æ•°å€¼ç¨³å®šæ€§
    print(f"Using device: {device}, dtype: {dtype}")

    # åŠ è½½processor
    print(f"ğŸ”„ æ­£åœ¨åŠ è½½å¤„ç†å™¨: {args.model_path}")
    processor = AutoProcessor.from_pretrained(args.model_path, use_fast=False)
    
    # æ£€æŸ¥å¹¶è®¾ç½®pad_token_idï¼Œç¡®ä¿paddingæ“ä½œçš„å®‰å…¨æ€§
    if processor.tokenizer.pad_token is None:
        print(f"âš ï¸  Warning: pad_token not set, using eos_token as pad_token")
        processor.tokenizer.pad_token = processor.tokenizer.eos_token
    print(f"âœ… pad_token_id: {processor.tokenizer.pad_token_id}")
    
    # æ ¹æ®from_pretrainedå‚æ•°å†³å®šæ¨¡å‹åŠ è½½æ–¹å¼
    if args.from_pretrained:
        print(f"ğŸ”„ æ­£åœ¨ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {args.model_path}")
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            args.model_path,
            torch_dtype=dtype,
            use_cache=False,  # ç¦ç”¨ KV ç¼“å­˜èŠ‚çœæ˜¾å­˜
            low_cpu_mem_usage=True,  # å‡å°‘ CPU å†…å­˜ä½¿ç”¨
        )
    else:
        print(f"ğŸ”„ æ­£åœ¨ä»åŸºç¡€æ¨¡å‹åŠ è½½æ¨¡å‹: {args.model_path}")
        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
            args.model_path,
            torch_dtype=dtype,
            use_cache=False,  # ç¦ç”¨ KV ç¼“å­˜èŠ‚çœæ˜¾å­˜
            low_cpu_mem_usage=True,  # å‡å°‘ CPU å†…å­˜ä½¿ç”¨
        )

    # è‹¥è¯è¡¨æœ‰æ‰©å±•ï¼Œåˆ™è°ƒæ•´åµŒå…¥çŸ©é˜µå¤§å°
    try:
        model.resize_token_embeddings(len(processor.tokenizer))
        try:
            model.config.vocab_size = int(len(processor.tokenizer))
        except Exception:
            pass
        print(f"âœ… å·²æ ¹æ® tokenizer è¯è¡¨å¤§å°é‡ç½®åµŒå…¥çŸ©é˜µ: {len(processor.tokenizer)}")
    except Exception as e:
        print(f"âš ï¸  é‡ç½®åµŒå…¥çŸ©é˜µå¤±è´¥: {e}")
    # ç¡®ä¿use_cacheä¸ºFalse
    model.config.use_cache = False

    # å…¨é‡å¾®è°ƒï¼šè®¾ç½®æ‰€æœ‰å‚æ•°ä¸ºå¯è®­ç»ƒ
    for param in model.parameters():
        param.requires_grad = True
    
    print(f"å…¨é‡å¾®è°ƒæ¨¡å¼ï¼šæ‰€æœ‰å‚æ•°å¯è®­ç»ƒ")
    print(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters() if p.requires_grad)}")
    
    # åœ¨ accelerator.prepare ä¹‹å‰å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if accelerator is not None:
        print(f"ğŸ”§ å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹...")
        model.gradient_checkpointing_enable()
        print(f"âœ… æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨: {model.is_gradient_checkpointing}")
    
    # ä½¿ç”¨acceleratorå‡†å¤‡æ¨¡å‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if accelerator is not None:
        model = accelerator.prepare(model)
        print(f"Model prepared with accelerator")
    
    if use_cuda:
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ. GPUå†…å­˜: {torch.cuda.memory_allocated() / 1024**3:.2f} GB / {torch.cuda.memory_reserved() / 1024**3:.2f} GB")
    else:
        print(f"æ¨¡å‹åŠ è½½å®Œæˆ. ä½¿ç”¨CPUè®­ç»ƒ")

    # åˆ›å»ºæ•°æ®é›†
    print(f"ğŸ”„ åˆ›å»ºæ•°æ®é›†...")
    train_dataset = LazyCoTDataset(
        processor=processor,
        cot_files=args.cot_files,
        base_dir=args.base_dir,
        max_samples=args.max_samples,
        max_target_length=args.max_target_length,
    )

    # è®¡ç®—å®é™…çš„è®­ç»ƒæ­¥æ•°
    if args.max_steps <= 0:
        # æ ¹æ®æ•°æ®é‡å’Œè®­ç»ƒé…ç½®è‡ªåŠ¨è®¡ç®—
        effective_batch_size = args.batch_size * args.gradient_accumulation_steps
        if accelerator is not None:
            effective_batch_size *= accelerator.num_processes
        
        total_samples = len(train_dataset)
        steps_per_epoch = total_samples // effective_batch_size
        if total_samples % effective_batch_size != 0:
            steps_per_epoch += 1
        
        calculated_max_steps = steps_per_epoch * args.epochs
        print(f"ğŸ“Š è‡ªåŠ¨è®¡ç®—è®­ç»ƒæ­¥æ•°:")
        print(f"   - æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   - æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {effective_batch_size}")
        print(f"   - æ¯è½®æ­¥æ•°: {steps_per_epoch}")
        print(f"   - æ€»è®­ç»ƒæ­¥æ•°: {calculated_max_steps}")
        
        # æ›´æ–°warmup_stepsï¼Œç¡®ä¿ä¸è¶…è¿‡æ€»æ­¥æ•°çš„50%
        if args.warmup_steps > calculated_max_steps * 0.5:
            args.warmup_steps = max(100, int(calculated_max_steps * 0.1))
            print(f"   - è°ƒæ•´é¢„çƒ­æ­¥æ•°: {args.warmup_steps}")
    else:
        calculated_max_steps = args.max_steps
        print(f"ğŸ“Š ä½¿ç”¨æŒ‡å®šçš„æœ€å¤§æ­¥æ•°: {calculated_max_steps}")

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        per_device_train_batch_size=args.batch_size,
        num_train_epochs=args.epochs,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,  # ä½¿ç”¨argsä¸­çš„æƒé‡è¡°å‡
        logging_steps=args.logging_steps,  # ä½¿ç”¨argsä¸­çš„æ—¥å¿—æ­¥æ•°
        save_steps=args.save_steps,  # ä½¿ç”¨argsä¸­çš„ä¿å­˜æ­¥æ•°
        fp16=False,  # ç¦ç”¨ FP16
        bf16=True,  # å¯ç”¨ BF16 æ··åˆç²¾åº¦è®­ç»ƒï¼Œä¸ accelerate config ä¿æŒä¸€è‡´
        max_grad_norm=args.max_grad_norm,  # ä½¿ç”¨argsä¸­çš„æ¢¯åº¦è£å‰ªé˜ˆå€¼
        save_total_limit=5,  # ä¿ç•™4ä¸ªæ£€æŸ¥ç‚¹
        logging_dir=args.log_dir,  # TensorBoard æ—¥å¿—ç›®å½•
        report_to="tensorboard",  # å¯ç”¨ TensorBoard
        dataloader_num_workers=0,  # è®¾ç½®ä¸º0ï¼Œé¿å…å¤šè¿›ç¨‹é—®é¢˜
        gradient_accumulation_steps=args.gradient_accumulation_steps,  # ä½¿ç”¨argsä¸­çš„æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        optim="adamw_torch",  # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼ŒDeepSpeedå…¼å®¹æ€§æ›´å¥½
        remove_unused_columns=False,  # ä¿ç•™æ‰€æœ‰åˆ—é¿å…æ•°æ®å¤„ç†å¼€é”€
        warmup_steps=args.warmup_steps,  # ä½¿ç”¨argsä¸­çš„é¢„çƒ­æ­¥æ•°
        lr_scheduler_type="cosine",  # ä½¿ç”¨ä½™å¼¦è°ƒåº¦å™¨ï¼Œæ›´é€‚åˆå…¨é‡å¾®è°ƒ
        dataloader_drop_last=False,  # ä¸ä¸¢å¼ƒæœ€åä¸€ä¸ªä¸å®Œæ•´çš„batch
        max_steps=calculated_max_steps,  # ä½¿ç”¨è®¡ç®—å‡ºçš„æœ€å¤§æ­¥æ•°
        # å†…å­˜ä¼˜åŒ–é€‰é¡¹
        dataloader_pin_memory=False,  # ç¦ç”¨pin_memoryèŠ‚çœæ˜¾å­˜
        gradient_checkpointing=False,  # å·²åœ¨ä»£ç ä¸­æ‰‹åŠ¨å¯ç”¨ï¼Œé¿å… DDP å†²çª
        # DeepSpeed é…ç½®
        deepspeed=args.deepspeed,  # DeepSpeed é…ç½®æ–‡ä»¶è·¯å¾„
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        data_collator=create_collate_fn(processor),  # ä½¿ç”¨å·¥å‚å‡½æ•°åˆ›å»ºcollate_fn
        callbacks=[TensorBoardCallback(log_dir=args.log_dir), GradientNaNCallback()],
    )



    print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    print(f"è®­ç»ƒé…ç½®: {args.epochs} epochs, batch_size={args.batch_size}, lr={args.learning_rate}")
    print(f"è®­ç»ƒæ­¥æ•°: {calculated_max_steps} steps, warmup_steps={args.warmup_steps}")
    
    # è®­ç»ƒæ¨¡å‹
    trainer.train()
    
    print("âœ… è®­ç»ƒå®Œæˆï¼æ­£åœ¨ä¿å­˜æ¨¡å‹...")
    
    # å›¾ç‰‡åŠ è½½ç»Ÿè®¡ï¼ˆå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼‰
    print(f"ğŸ“Š å›¾ç‰‡åŠ è½½ç»Ÿè®¡:")
    print(f"   - å»¶è¿ŸåŠ è½½æ¨¡å¼ï¼šå›¾ç‰‡æŒ‰éœ€åŠ è½½ï¼Œæ— ç¼“å­˜")
    print(f"   - æ€»æ ·æœ¬æ•°: {len(train_dataset)}")
    
    # ä¿å­˜æ¨¡å‹å’Œprocessor
    if accelerator is not None:
        # å¦‚æœä½¿ç”¨acceleratorï¼Œç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
        accelerator.wait_for_everyone()
        # åªåœ¨ä¸»è¿›ç¨‹ä¸Šä¿å­˜æ¨¡å‹å’Œprocessor
        if accelerator.is_main_process:
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {args.output_dir}")
            trainer.save_model(args.output_dir)
            
            print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜processoråˆ°: {args.output_dir}")
            try:
                processor.save_pretrained(args.output_dir)
                print(f"âœ… Processorä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âŒ Processorä¿å­˜å¤±è´¥: {e}")
            
            # éªŒè¯å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            required_files = ["preprocessor_config.json", "tokenizer_config.json", "tokenizer.json"]
            for file in required_files:
                file_path = os.path.join(args.output_dir, file)
                if os.path.exists(file_path):
                    print(f"âœ… {file} å­˜åœ¨")
                else:
                    print(f"âŒ {file} ç¼ºå¤±")
            
            print(f"ğŸ‰ æ¨¡å‹å’Œprocessorå·²ä¿å­˜åˆ°: {args.output_dir}")
    else:
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ¨¡å‹åˆ°: {args.output_dir}")
        trainer.save_model(args.output_dir)
        
        print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜processoråˆ°: {args.output_dir}")
        try:
            processor.save_pretrained(args.output_dir)
            print(f"âœ… Processorä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"âŒ Processorä¿å­˜å¤±è´¥: {e}")
        
        # éªŒè¯å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        required_files = ["preprocessor_config.json", "tokenizer_config.json", "tokenizer.json"]
        for file in required_files:
            file_path = os.path.join(args.output_dir, file)
            if os.path.exists(file_path):
                print(f"âœ… {file} å­˜åœ¨")
            else:
                print(f"âŒ {file} ç¼ºå¤±")
        
        print(f"ğŸ‰ æ¨¡å‹å’Œprocessorå·²ä¿å­˜åˆ°: {args.output_dir}")


if __name__ == "__main__":
    main()
